{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from fuzzywuzzy import process\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login for the database and server\n",
    "user = 'your_user'\n",
    "password = 'your_password'\n",
    "host = 'localhost'\n",
    "port_number = '5432'\n",
    "database_name = 'Haven'\n",
    "connection_string = f'postgresql://{user}:{password}@{host}:{port_number}/{database_name}'\n",
    "engine = create_engine(connection_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new function for loading the data from postgres\n",
    "def load_data(table_name): #plug in relevant names\n",
    "    return pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full code that recognizes duplicates both within a table itself and with other tables\n",
    "def deduplicate_data(df, column_name, threshold=90):\n",
    "    # Convert all values in the column to strings\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Fill missing values with a placeholder string\n",
    "    df[column_name].fillna('', inplace=True)\n",
    "    \n",
    "    unique_names = pd.Series(df[column_name].unique())\n",
    "    matches = unique_names.apply(lambda x: process.extract(x, unique_names, limit=2))\n",
    "    matches = matches.apply(lambda x: [(m[0], m[1]) for i, m in enumerate(x) if m[1] >= threshold and m[0] != unique_names.iloc[i]])\n",
    "    \n",
    "    # Consolidate duplicates\n",
    "    consolidated = {}\n",
    "    for i, match_list in enumerate(matches):\n",
    "        for match in match_list:\n",
    "            original = unique_names[i]\n",
    "            duplicate = match[0]\n",
    "            if duplicate in consolidated:\n",
    "                consolidated[duplicate].add(original)\n",
    "            else:\n",
    "                consolidated[duplicate] = {original, duplicate}\n",
    "    \n",
    "    # Create a mapping of duplicates to originals\n",
    "    mapping = {old: new for new, old_set in consolidated.items() for old in old_set}\n",
    "    \n",
    "    # Apply mapping to the column\n",
    "    df[column_name] = df[column_name].replace(mapping)\n",
    "    \n",
    "    # Convert all values in the column back to strings (in case any values were converted to other types during the operation)\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Iterate through each row and replace older cells with newer ones if there are differences\n",
    "    for index, row in df.iterrows():\n",
    "        duplicate_rows = df[df[column_name] == row[column_name]]\n",
    "        if len(duplicate_rows) > 1:  # If there are duplicates\n",
    "            newer_row = duplicate_rows.iloc[-1]  # Get the newer row\n",
    "            for col in df.columns:  # Iterate through each column\n",
    "                if row[col] != newer_row[col]:  # If the cell values are different\n",
    "                    df.at[index, col] = newer_row[col]  # Replace with the newer value\n",
    "    \n",
    "    # Drop duplicate rows (keeping the last occurrence)\n",
    "    df.drop_duplicates(subset=[column_name], keep='last', inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the data into a new grouped table\n",
    "\n",
    "#whichever new table should be on df2 and the old table should be df1\n",
    "def main():\n",
    "    df1 = load_data(' ')\n",
    "    df2 = load_data(' ')\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    deduplicated_df = deduplicate_data(combined_df, 'Names')\n",
    "    deduplicated_df.to_sql('groupcal6', engine, if_exists='replace', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to pull back the new grouped table and turn it into a csv file so it would go through the Consolidated Data Cleaner.ipynb\n",
    "\n",
    "\n",
    "# filepath to where you want to extract the csv to so that you can use the next python script\n",
    "filepath = ' '\n",
    "\n",
    "# Extract directory path from the filepath\n",
    "directory = os.path.dirname(filepath)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Make a dataframe from the loaded grouped table data\n",
    "df4=load_data('groupcal2')\n",
    "\n",
    "# Save the grouped table into your desktop as a csv file that can be read for the python script\n",
    "df4.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
